---
title: "Analyze objects"
---

```{r include = FALSE}
knitr::opts_knit$set(root.dir = "E:/Desktop/UFSC/cursos/pliman_tut/imgs")
```


# Directory
```{r eval=FALSE}
setwd("E:/Desktop/UFSC/cursos/pliman_tut/imgs")
```



# Working with polygons
> A 'polygon' is a plane figure that is described by a finite number of straight line segments connected to form a closed polygonal chain (Singer, 1993)^[Singer, M.H. 1993. A general approach to moment calculation for polygons and line segments. Pattern Recognition 26(7): 1019–1028. doi: 10.1016/0031-3203(93)90003-F.].

Given the above, we can conclude that image objects can be expressed as polygons with `n` vertices. `pliman` has a set of functions(`draw_*()`) useful for drawing common shapes like circles, squares, triangles, rectangles and `n`- tagons . Another group of ` poly_*()` functions can be used to analyze polygons. Let's start with a simple example related to the area and perimeter of a square.

```{r fig.height =6}
library(pliman)
square <- draw_square(side = 1)
poly_area(square)
poly_perimeter(square)
```


Now, let's see what happens when we start with a hexagon and increase the number of sides up to 1000.

```{r fig.height =6}
shapes <- list(  side6 = draw_n_tagon(6, plot = FALSE),
                 side12 = draw_n_tagon(12, plot = FALSE),
                 side24 = draw_n_tagon(24, plot = FALSE),
                 side100 = draw_n_tagon(100, plot = FALSE),
                 side500 = draw_n_tagon(500, plot = FALSE),
                 side100 = draw_n_tagon(1000, plot = FALSE))

plot_polygon(shapes, merge = FALSE)

poly_area(shapes)
poly_perimeter(shapes)
```


Note that when $n \to \infty$, the sum of the sides becomes the circumference of the circle, given by $2 \pi r$, and the area becomes $\pi r^2$. This is fun, but `pliman` is primarily designed for analyzing plant image analysis. So why use polygons? Let's see how we can use these functions to get applicable information.


```{r fig.width =8, fig.height =6}
leaves <- image_import("ref_leaves.jpg", plot = TRUE)

# getting the outline of objects
cont <- object_contour(leaves, watershed = FALSE)
# plotting the polygon
plot_polygon(cont)
```


Nice! We can use the contours of any object to get useful information related to its shape. To reduce the amount of output, I will only use five samples: 1, 3, 12, and 24.

```{r fig.height =6}
cont <- cont[c("1", "3", "12", "24")]
plot_polygon(cont)

```

In the current version of `pliman`, you can calculate the following measurements. For more details, see Chen & Wang (2005)^[Chen, C.H., and P.S.P. Wang. 2005. Handbook of Pattern Recognition and Computer Vision. 3rd ed. World Scientific.], Claude (2008)^[Claude, J. 2008. Morphometrics with R. Springer.], and Montero et al. (2009)^[Montero, R.S., E. Bribiesca, R. Santiago, and E. Bribiesca. 2009. State of the Art of Compactness and Circularity Measures. International Mathematical Forum 4(27): 1305–1335.].

## Area

The area of a shape is calculated using Shoelace 's formula (Lee and Lim, 2017)^[Lee, Y., and W. Lim. 2017. Fórmula de cadarço: conectando a área de um polígono e o produto vetorial vetorial. The Mathematics Teacher 110(8): 631–636. doi: 10.5951/MATHTEACHER.110.8.0631.], as follows

$$
A=\frac{1}{2}\left |\sum_{i=1}^{n}\left(x_{i} y_{i+1}-x_{i+1}y_{i}\right)\right|
$$

```{r fig.height =6}
poly_area(cont)
```


## Perimeter
The perimeter is calculated as the sum of the Euclidean distance between all points on a shape. Distances can be obtained with `poly_distpts()`.

```{r}
poly_perimeter(cont)

# perimeter of a circle with radius 2
circle <- draw_circle(radius = 2, plot = FALSE)
poly_perimeter(circle)

# check the result
2*pi*2
```


## Radius

The radius of a pixel on the object's contour is calculated as its distance from the object's centroid(also called 'center of mass'). These distances can be obtained with `poly_centdist()`.

```{r}
dist <- poly_centdist(cont)

# stats for radius
mean_list(dist)
min_list(dist)
max_list(dist)
sd_list(dist)

# average radius of circle above
poly_centdist(circle) |> mean_list()
```



## Length and width

The length and width of an object are calculated with `poly_lw()`, as the difference between the maximum and minimum of the `x` and `y` coordinates after the object has been aligned with `poly_align()`.

```{r fig.height =2, fig.width=10}
poly_lw(cont)
```

## Circularity, eccentricity, diameter and elongation

Circularity(Montero et al. 2009)^[Montero, R.S., E. Bribiesca, R. Santiago, and E. Bribiesca. 2009. State of the Art of Compactness and Circularity Measures. International Mathematical Forum 4(27): 1305–1335] is also called shape compactness, or measure of roundness of an object. It is given by $C = P^2 / A$, where $P$ is the perimeter and $A$ is the area of the object.

```{r}
poly_circularity(cont)
```

As the above measurement depends on the scale, normalized roundness can be used. In this case, a perfect circle is assumed to have circularity equal to 1. This measure is invariant under translation, rotation and scale transformations, given $Cn = P^2 / 4 \pi A$

```{r}
poly_circularity_norm(cont)

# normalized circularity for different shapes
draw_square(plot =FALSE) |> poly_circularity_norm()
draw_circle(plot=FALSE) |> poly_circularity_norm()
```


` poly_circularity_haralick()` calculates the circularity of Haralick , CH(Haralick , 1974)^[Haralick, R.M. 1974. A Measure for Circularity of Digital Figures. IEEE Transactions on Systems, Man, and Cybernetics SMC-4(4): 394–396. doi: 10.1109/TSMC.1974.5408463.]. The method is based on calculating all Euclidean distances from the object's centroid to each contour pixel. With this set of distances, the mean($m$) and the standard deviation($s$) are calculated. These statistical parameters are used in a ratio that calculates CH as $CH = m/ sd $.

```{r}
poly_circularity_haralick(cont)
```

`poly_convexity()` Calculates the convexity of a shape using a ratio of the perimeter of the convex hull to the perimeter of the polygon.

```{r}
poly_convexity(cont)
```


`poly_eccentricity()` Calculates the eccentricity of a shape using the ratio of the eigenvalues(coordinate inertia axes).

```{r}
poly_eccentricity(cont)
```


`poly_elongation()` Calculates the elongation of an object as `1 - width / length`

```{r}
poly_elongation(cont)
```


`poly_caliper()` Calculates the gauge(also called Feret's diameter).

```{r}
poly_caliper(cont)
```


Users can use the `poly_measures()` function to calculate most object measurements in a single call.

```{r}
measures <- poly_measures(cont) |> round_cols()
t(measures)
```

If the image resolution is known, the measurements can be corrected with ` get_measures()`. Image resolution can be obtained using a known distance in the image. In the example, the white square has a side of 5 cm. So using `dpi()` the resolution can be obtained. In this case, the dpi is ~50.

```{r}
(color_measures <- get_measures(measures, dpi = 50))

```


Some useful functions can be used to manipulate coordinates. In the following example I will show some features implemented in `pliman`. Just for simplicity, I'll just use object 2.


```{r}
o2 <- cont[["3"]]
plot_polygon(o2)
```

## Rotate polygons

` poly_rotate()` can be used to rotate the polygon coordinates by an `angle` (0-360 degrees) in the trigonometric (anti-clockwise) direction.


```{r fig.width =4, fig.height =4}
rot <- poly_rotate(o2, angle = 45)
```


## Invert polygons
` poly_flip_x()` and ` poly_flip_y()` can be used to flip shapes along the x and y axis, respectively.

```{r fig.width =8, fig.height =4}
flipped <- 
  list(fx = poly_flip_x(o2), 
       fy = poly_flip_y(o2))

plot_polygon(flipped, merge = FALSE, aspect_ratio = 1)
```


## Perimeter sampling

`poly_sample()` samples `n` coordinates between existing points, and `poly_sample_prop()` samples a proportion of coordinates between existing ones.

```{r fig.width =4, fig.height =4}
# sample 50 coordinates
poly_sample(o2, n=10) |> plot_polygon()

# sample 10% of coordinates
poly_sample_prop(o2, prop = 0.1) |> plot_polygon()
```


## smoothing

`poly_smooth()` smooths the contour of a polygon by combining ` prop` coordinate point samples and interpolating them using ` vertices` vertices(default is 1000) .

```{r fig.width =6, fig.height = 6}
smoothed <-
  list( original = o2,
        s1 = poly_smooth(o2, prop = 0.2, plot = FALSE),
        s2 = poly_smooth(o2, prop = 0.1, plot = FALSE),
        s1 = poly_smooth(o2, prop = 0.04, plot = FALSE)
  )

plot_polygon(smoothed, merge = FALSE, ncol = 2, aspect_ratio = 1)
```


## Noises


`poly_jitter()` adds a small amount of noise to a set of coordinates. See `base::jitter()` for more details.

```{r fig.width =8, fig.height =4}
set.seed(1)
c1 <- draw_circle(n = 200, plot = FALSE)
c2 <- draw_circle(n = 200, plot = FALSE) |>
  poly_jitter(noise_x = 100,
              noise_y = 100,
              plot = FALSE)

plot_polygon(list(c1, c2), merge = FALSE)
```




# Analyzing objects

The functions seen so far can be used to obtain measurements of objects. However, for image analysis it is necessary to combine different functions (mainly `object_contour()` and `poly_measures()`). Also, almost always, several images need to be analyzed and repeating this process each time would be tedious and inefficient. To address these needs, users can use the ` analyze_objects()` function. Let's start with a simple example, using the `object_300dpi.png` image available on [ GitHub page](https://github.com/TiagoOlivoto/pliman/tree/master/inst/tmp_images). To facilitate importing images from this folder, an `image_pliman()` helper function is used.


```{r collapse =TRUE}
library(pliman)
library(tidyverse)
library(metan)

img <- image_pliman("objects_300dpi.jpg", plot = TRUE)

```



The image above was produced with Microsoft PowerPoint. It has a known resolution of 300 dpi(dots per inch) and displays four objects

- Larger square: 10 x 10 cm (100 cm$^2$)
- Smaller square: 5 x 5 cm(25 cm$^2$)
- Rectangle: 4 x 2 cm(8 cm$^2$)
- Circle: 3 cm in diameter(~7.07 cm$^2$)


To count the objects in the image we use `analyze_objects()` and inform the image(the only required argument). By default, the `NB` index is used for object segmentation.


```{r, fig.width = 10, fig.height = 5}
img_res <- analyze_objects(img, marker = "id")
```



## Adjusting object measurements

The results were stored in `img_res`. Since there is no scale declared in the example above, we have no idea about the actual area of objects in cm$^2$, only in pixels. In this case, we use `get_measures()` to adjust pixel measurements to metric units.

There are two main ways to adjust object measurements (from pixels to cm, for example). The first is to declare the known area, perimeter or radius of a given object. The measure for the other objects will then be calculated by a simple rule of three. The second is by declaring a known image resolution in dpi(dots per inch). In this case, perimeter, area and radius will be adjusted by the dpi informed.

### Declaring a known value

Since we know the area of the larger square (object 1), let's adjust the area of the other objects in the image using this.


```{r}
get_measures(img_res ,
             id = 1,
             area ~ 100)

```



### Declaring the image resolution

If the image resolution is known, all measurements will be adjusted accordingly. Let's see a numerical example with `pixels_to_cm()`. This function converts the number of pixels(* px *) into cm, considering the image resolution in ` dpi `, as follows: $cm = px \times(2.54 / dpi)$. As we know the number of pixels of the larger square, its perimeter in cm is given by



```{r}
# number of pixels for the perimeter of the largest square

ls_px <- img_res$results$perimeter [1]
pixels_to_cm(px = ls_px , dpi = 300)


```

The perimeter of object 1 adjusted by image resolution is very close to the known value (40 cm). Below, the values of all measures are adjusted by declaring the `dpi` argument in `get_measures()`.

```{r}
img_res_cor <- get_measures(img_res , dpi = 300)

t(img_res_cor)

```



### Understanding measurements

```{r}
object_contour(img) %>% # get the contour of objects
  poly_mass() %>% # computes center of mass and minimum and maximum radii
  plot_mass() # plot the measurements
```

* Larger square:
- The minimum diameter(a = 9.97) can be considered as the side of the square
- The maximum diameter, given by $a \sqrt {2}$ can be considered the diagonal of the square ($9.97 \sqrt {2} = 14,099 \approx 14,105$

```{r}
t(img_res_cor)
```


## Texture features

The function computes 13 Haralick texture features for each object based on a gray-level co-occurrence matrix (Haralick et al. 1979)^[Haralick, R.M., K. Shanmugam, and I. Dinstein. 1973. Textural Features for Image Classification. IEEE Transactions on Systems, Man, and Cybernetics SMC-3(6): 610–621. doi: 10.1109/TSMC.1973.4309314]. Haralick features depend on the configuration of the parameters `har_nbins` and `har_scales`. `har_nbins` controls the number of bins used to compute the Haralick matrix. A smaller `har_nbins` can give more accurate estimates of the correlation because the number of events per bin is higher. While a higher value will give more sensitivity. `har_scales` controls the number of scales used to compute the Haralick features. Since Haralick features compute the correlation of intensities of neighboring pixels, it is possible to identify textures with different scales, e.g., a texture that is repeated every two pixels or 10 pixels. By default, the Haralick features are computed with the R band. To chance this default, use the argument `har_band`. For example, `har_band = 2` will compute the features with the green band. 

The followig measures are returned (fore more details, see [this post](https://earlglynn.github.io/RNotes/package/EBImage/Features-Haralick.html))

- `asm`: The angular second-moment feature.
- `con`: The contrast feature
- `cor`: Correlation measures the linear dependency of gray levels of
neighboring pixels.
- `var`: The variance of gray levels pixels.
- `idm`: The Inverse Difference Moment (IDM), i.e., the local
homogeneity.
- `sav`: The Sum Average.
- `sva`: The Sum Variance.
- `sen`: Sum Entropy.
- `dva`: Difference Variance.
- `den`: Difference Entropy
- `f12`: Difference Variance.
- `f13`: The angular second-moment feature.


## Single image processing
The `analyze_objects()` function calculates a range of measurements that can be used to study the shape and texture of objects, such as leaves. In the following example, I show how to plot the length and width of each leaf in the following image.


```{r}
leaves <- image_import("folhas.jpg", plot = TRUE)

leaves_meas <-
  analyze_objects(leaves ,
                  watershed = FALSE,
                  col_background = "black")

leaves_cor <- get_measures(leaves_meas , dpi = 300)

t(leaves_cor)


# plot width and length
plot_measures(leaves_cor , measure = "width")
plot_measures(leaves_cor , measure = "length", vjust = 50)
```



Here, we will count the grains in the `grains.jpg` image. This image has a cyan background and contains 90 soybeans that touch each other. The ` analyze_objects()` function segments the image using the normalized blue index by default, as follows $NB =(B /(R + G + B))$, where *R*, *G* and *B * are the red, green and blue bands. Note that if the image is contained in the default directory, it is not necessary to import it. Just enter the image name in quotes in the `img` argument(e.g., `img = "grains"`).

In this example, objects are counted and segmented objects are colored with random colors using the `show_segmentation = TRUE` argument. Users can set ` show_contour = FALSE` to remove the contour line and identify the objects (in this example, the grains) using the `marker = "id"` arguments. The background color can also be changed with `col_background`.



```{r, fig.width = 12, fig.height = 6}
count <-
  analyze_objects("grains",
                  show_segmentation = TRUE,
                  show_contour = FALSE,
                  marker = "id")
count$statistics
```


```{r}

# Get the measurements of the object
measurements <- get_measures(count)
head(measurements)
```


In the following example, we will select objects with an area above the average of all objects using ` lower_size = 719.1`.



```{r, fig.width = 12, fig.height = 6}

analyze_objects("grains",
                marker = "id",
                lower_size = 719.1)
```



Users can also use the `topn _*` arguments to select the `n` objects based on the smallest or largest areas. Let's see how to select the 5 grains with the smallest area, showing the original grains on a blue background. We will also use the `my_index` argument to choose a custom index to segment the image. Just for comparison, we will explicitly set the normalized blue index by calling `my_index = "B /(R + G + B)"`.



```{r, fig.width = 12, fig.height = 6}
analyze_objects("grains",
                marker = "id",
                topn_lower = 5,
                col_background = "salmon",
                my_index = "B /(R + G + B)") # normalized blue(NB)
```



## Batch processing

In image analysis, it is often necessary to process more than one image. For example, in plant breeding, the number of grains per plant(eg wheat) is often used in indirect selection of high-yielding plants. In `pliman`, batch processing can be done when the user declares the `pattern` argument.


To speed up processing time, especially for large numbers of images, the `parallel = TRUE` argument can be used. In this case, images are processed asynchronously (in parallel) in separate `R` sessions running in the background on the same machine. The number of sections is set to 50% of available cores. This number can be explicitly controlled with the `workers` argument.


```{r}
system.time(
  list_res <- analyze_objects(pattern = "img_sb", show_image = FALSE)
)

# parallel processing
# 6 multiple sections (50% of my pc's cores)
system.time(
  list_res <-
    analyze_objects(pattern = "img_sb",
                    show_image = FALSE,
                    parallel = TRUE)
)

```




# Object coordinates
Users can get the coordinates for all desired objects with `object_coord()`. When the `id` argument is set to `NULL` (default), a bounding rectangle is drawn including all objects. Use `id = "all"` to get the coordinates of all objects in the image or use a numeric vector to indicate the objects to calculate the coordinates. Note that the `watershed = FALSE` argument is used to avoid unique objects being split up into multiple objects by the watershed segmentation algorithm.


```{r objec2}
leaves <- image_import("folhas.jpg", plot = TRUE)

# get the id of each object
object_id(leaves, watershed = FALSE)

# Get the coordinates of a bounding rectangle around all objects
object_coord(leaves, watershed = FALSE)

# Get coordinates for all objects
object_coord(leaves ,
             id = "all",
             watershed = FALSE)

# Get the coordinates of objects 1 and 2
# 20 border pixels
object_coord(leaves ,
             id = 1,
             edge = 20,
             watershed = FALSE)
```



# Isolating objects

Knowing the coordinates of each object, it is possible to isolate it. The `object_isolate()` function is used for this. In the following example, I will isolate object 1 and set a 10-pixel border around the object.

```{r object3}
id1 <-
  object_isolate(leaves ,
                 watershed = FALSE,
                 id = 1,
                 edge = 10)
plot(id1)
```

# Including objects in a list

`object_split()` can be used to split up a series of objects contained in a single image into a list, where each element is one object. By default, the background is removed and shown in white.

```{r fig.width =10}
list <- object_split(leaves, watershed = FALSE)
str(list)
```



# RGB values for each object

To get the RGB intensity of each object in the image, we use the `object_rgb = TRUE` argument in the `analyze_objects()` function. In the following example, we will use the R, G and B bands and their normalized values. The `pliman_indexes()` function returns the indexes available in the package. To compute a specific index, simply enter a formula containing the values of R, G, or B (e.g. `object_index = "B/G+R"`).



```{r rgb2, fig.width = 10, fig.height = 6}
img <- image_import("green.jpg", plot = TRUE)
(indx <- pliman_indexes())

soy_green <-
  analyze_objects(img ,
                  object_index = indx [1:6], # R:NB
                  marker = "id",
                  marker_col = "black",
                  col_background = "white",
                  show_contour = FALSE)

# PCA with the indices
ind <- summary_index(soy_green, type ="var")

```

The `R` index provided the greatest contribution to the variation of PC1. The biplot containing the indices (variables) and the grains (individuals) can be seen below.

```{r fig.width =8, fig.height =8}
get_biplot(ind$pca_res, show = "var")
get_biplot(ind$pca_res, show = "ind")

```


Now, let's plot the `R` index on each object

```{r}
plot(img)
plot_measures(soy_green ,
              measure = "R",
              col = " black ")
```


It seems that grains with average red (`R`) value less than 0.6 can be considered greenish seeds. Users can then work with this feature and adapt it to their case.

```{r rgb4, fig.width =10, fig.height =4}
report <-
  summary_index(soy_green ,
                index = "R",
                cut_point = 0.6,
                plot = FALSE)
ids <- report$ids
report$between_id
report$within_id[ids,]


# ratio of pixels of each object(above and below 0.6)
barplot(t(report$within_id [,6:7]) |> as.matrix(),
        legend = c("R < 0.6", "R > 0.6"))
```


In the following graph, I plot the distribution of the greenish and non-greenish R, G, and B values.

```{r}
# distribution of RGB values
rgbs <-
  soy_green$object_rgb |>
  mutate(type = ifelse(id %in% ids, " Greenish ", " No greenish ")) |>
  select(-id) |>
  pivot_longer(-type)

ggplot(rgbs, aes(x = value)) +
  geom_density(aes(fill = name), alpha = 0.5) +
  facet_wrap(~ type)


```

Now, using the ids of each grain, I plot the values only in the greenish ones.


```{r}
# plot 
plot(img)
plot_measures(soy_green ,
              id = ids,
              measure = "R",
              col = "black")
cont <- object_contour(img , show_image = FALSE)

plot_contour(cont,
             id = ids,
             col = "red")

```


::: {.callout-tip}
When there are many objects, the `parallel = TRUE` argument will speed up extracting the RGB values. In the following example, an image with 1343 grains of *Vicia cracca* is analyzed. The indices `"R"` and `"R/G"` are computed. Grains with an average red value greater than 0.25 are highlighted.

```{r rgb5, eval=FALSE}
library(pliman)
img2 <- image_import("vicia.jpg")

vicia <-
  analyze_objects(img2,
                  index = "B",
                  object_index = "R",
                  show_image = FALSE,
                  parallel = TRUE)

summary_index <-
  summary_index(vicia ,
                index = "R",
                cut_point = 0.25,
                select_higher = TRUE)

count2 <-
  object_contour(img2,
                 index = "B",
                 show_image = FALSE)

ids2 <- summary_index$ids
plot_contour(count2, col = "red")

```

:::




# Leaf area
## Known resolution

```{r}
leaves <- image_import(image = "ref_leaves.jpg", plot = TRUE)

af <-
  analyze_objects(leaves ,
                  watershed = FALSE,
                  show_contour = FALSE,
                  col_background = "black",
                  marker = "id")
af_cor <- get_measures(af, dpi = 50.5)

plot_measures(af_cor ,
              measure = "area",
              vjust = -30,
              col = " red ")
```



## Reference object (dev version)
### Single images
The `reference` argument can now be used to correct the object measures even when images with different shooting distances are used. In this example, the leaf area of the `ref_leaves` image is quantified and corrected considering a 5 x 5 (25 cm$^2$) white square as the reference object. For this, it is necessary to provide color palettes referring to the background (`background`), leaves (` foreground`) and the reference object (`reference`). Also, the area of the reference object needs to be informed in `reference_area`.

```{r}
library(pliman)
img <- image_import(pattern = "ref_", plot = TRUE)

area <-
  analyze_objects(img = "ref_leaves",
                  foreground = "ref_folha",
                  background = "ref_back",
                  reference = "ref_ref",
                  reference_area = 25,
                  marker = "area",
                  watershed = FALSE)
```



### Multiple images
```{r}
res <-
  analyze_objects(pattern = "img_",
                  dir_original = "linhaca",
                  foreground = "folhalin",
                  background = "fundolin",
                  reference = "reflin",
                  reference_area = 20,
                  watershed = FALSE,
                  filter = 3)

area <- 
  res$results |> 
  separate_col(img, 
               into = c("img", "avaliacao", "block"))


df_plot <- 
  area |> 
  sum_by(avaliacao, block, .vars = area) |> 
  group_by(avaliacao) |> 
  desc_stat(area, stats = c("mean, sd.amo"))




ggplot(df_plot, aes(avaliacao, mean)) + 
  geom_point() +
  geom_errorbar(aes(ymin = mean - sd.amo, 
                    ymax = mean + sd.amo),
                width = 0.2) +
  labs(x = "Período de avaliação",
       y = "Área foliar (cm2 por planta)") +
  theme_bw(base_size = 16)
```





## Filling 'holes'

An important aspect to consider in leaf area measures is when leaves present 'holes'. This can occur, for example, by the attack of pests. In this case, the area would have to be considered, because it was there! The image bellow is used as an example.

```{r fig.width =10, fig.height =6}
holes <- image_import("holes.jpg", plot = TRUE)

```


In this case, the missing area will not be computed using the default settings of `analyze_objects()`. To include this area as the leaf area, we can use the argument `fill_hull()`. Note that this will only work for missing areas within a closed object. If the missing area includes the original leaf contour, there is no (yet available) way to reconstruct the leaf perimeter.


```{r fig.width =10, fig.height =6}

af <-
  analyze_objects(holes,
                  watershed = FALSE,
                  col_background = "white",
                  marker = "area",
                  marker_col = "red",
                  marker_size = 3,
                  show_image = FALSE,
                  save_image = TRUE,
                  dir_processed = tempdir(),
                  contour_size = 5)

# fill the missing area
af2 <-
  analyze_objects(holes,
                  fill_hull = TRUE, # fill ' holes '
                  watershed = FALSE,
                  col_background = "white",
                  marker = "area",
                  marker_col = "red",
                  marker_size = 3,
                  show_image = FALSE,
                  save_image = TRUE,
                  prefix = "proc2_",
                  dir_processed = tempdir(),
                  contour_size = 5)

imgs <- image_import(pattern = "proc", path = tempdir())
image_combine(imgs)
```

We can simply use the ratio between `proc_img` and `proc_img2` to compute the injured area in this leaflet.

```{r}
# percent of injuried area
100 - 88379 / 99189 * 100
```




## Compound leaves

A simple leaf blade is undivided. The blade of a compound leaf is divided into several leaflets. In the following examples, I will show how to analyze simple and compound leaves with `analyze_objects()`, mainly if the goal is to obtain the measures for each leaf (e.g., mean area), where the number of objects (leaves) will influence the results.

The following images by [Daniel Saueressig](https://www.florestaombrofilamista.com.br/sidol/?menu=contact) were obtained from the *Sistema de Identificação Dendrológica Online - Floresta Ombrófila Mista*^[https://www.florestaombrofilamista.com.br/sidol/?menu=glossary] and show examples of simple and compound leaves.

```{r sc1, fig.width=10}
imgs <- 
  image_import(c("simple.jpg", "compound.jpg"),
               plot = TRUE)

```


Analyzing non-touching simple leaves is fairly simple. We already did that. The squares in the background have 4 cm$^2$. With this information, it is possible to obtain the image resolution with `dpi(simple)`, which will be useful to adjust the measures. In this case, the estimated dpi is 48.65.

```{r sc2}
simple <- imgs$simple.jpg
sarea <- analyze_objects(simple, marker = "id")

```

Note that with the default settings, the simple leaf was partitioned into small, segmented leaves. This can be solved by either using `object_size = "large"` or `watershed = FALSE`, to omit the watershed segmentation algorithm. The last is used here.


```{r sc3}
sarea <- 
  analyze_objects(simple,
                  watershed = FALSE,
                  marker = "id",
                  show_chull = TRUE)
sarea_cor <- get_measures(sarea, dpi = 48.65)
sarea_cor
```


For compound leaves, if the watershed segmentation is used, leaflets will probably be considered as different leaves, as can be seen below.

```{r sc4}
compound <- imgs$compound.jpg
carea <- 
  analyze_objects(compound,
                  show_segmentation = TRUE,
                  show_contour = FALSE,
                  marker = "id")
```

Therefore, using `watershed = FALSE` will solve this problem, since all leaflets connected by at least one pixel will be considered part of the same leaf.

```{r sc5}
carea <- 
  analyze_objects(compound,
                  watershed = FALSE,
                  show_segmentation = TRUE,
                  show_contour = FALSE,
                  show_chull = TRUE,
                  marker = "id")
carea_cor <- get_measures(carea, dpi = 49.5)
carea_cor
```




## Multiple images of the same sample

If users need to analyze multiple images from the same sample, the images must share the same filename prefix, which is defined as the part of the filename that precedes the first hyphen (`-`) or underscore (`_`). Then, when using ` get_measures()`, measurements from leaf images called, for example, `F1-1.jpeg`, `F1_2.jpeg` and `F1-3.jpeg` will be combined into a single image (`F1`), displayed in the `merge` object. This is useful, for example, for analyzing large sheets that need to be split into multiple images or multiple sheets belonging to the same sample that cannot be scanned into a single image.

In the following example, five images will be used as examples. Each image has leaves of different species. The images have been split into different images that share the same prefix (eg L1_\*, L2_\*, and so on). Note that to ensure that all images are processed, all images must share a common pattern, in this case (`"L"`). The three dots in the lower right corner have a known distance of 5 cm between them, which can be used to extract the dpi of the image with `dpi()`. For teaching purposes only, I will assume that all images are 100 dpi resolution .


```{r merge0, fig.width = 10, fig.height = 10}
# entire images
imgs <-
  image_import(pattern = "leaf",
               plot = TRUE,
               ncol = 2)

# images of the same sample
sample_imgs <-
  image_import(pattern = "L",
               plot = TRUE,
               ncol = 5)
```

Here, I will use the `pattern = "L"` to indicate that all images with this pattern name should be merged. The green index (`"G"`) is used to segment the leaves and `watershed = FALSE` is used to omit the watershed segmentation algorithm.


```{r merge1}
merged <-
  analyze_objects(pattern = "L",
                  index = "B",
                  watershed = FALSE)
```

Using the `get_measures()` function it is possible to convert measurements from pixel units to metric units(cm$^ 2$).

```{r merge2}
merged_cor <- get_measures(merged, dpi = 100)
```

Note that the  merged_cor` is a list with three objects:

* `results`: a data frame that contains the measurements of each individual object (in this case, leaf) of each analyzed image.

```{r merge3}
merged_cor$results
```

* `summary` : a data frame that contains the summary of the results, containing the number of objects in each image (`n`) the sum, mean and standard deviation of the area of each image, as well as the average value for all others measurements (perimeter, radius, etc.)


```{r merge4}
merged_cor$summary

```

* `merge`: a data frame that contains the results merged by image prefix. Note that in this case the results are displayed by L1, L2, L3, L4 and L5.

```{r merge5}
merged_cor$merge

```

The ` area_sum` of img `L1` is the sum of the two sheets (one in each image).

```{r merge6}
sum(merged_cor$results$area [1:2])

```


Below, I will create a plot to show the distribution of leaf area  

```{r merge9, fig.width =10, fig.height =5}
df_leaf <-
  merged_cor$results %>%
  separate(img , into = c("img", "code"))

# leaf area of the different species
p1 <-
  ggplot(df_leaf, aes(img, area)) +
  geom_boxplot() +
  geom_jitter(color="red") +
  labs(x = " Image ", y = expression(Area ~(cm^2)))

p2 <-
  ggplot(df_leaf , aes(x = img , y = area)) +
  stat_summary(fun = sum,
               geom = "bar",
               col = " black ") +
  labs(x = " Image ", y = expression(Total~area ~(cm^2)))


# solidity of the different species
p3 <-
  ggplot(df_leaf , aes(x = img , y = solidity)) +
  geom_boxplot() +
  geom_jitter(color="red") +
  labs(x = "Image", y = "Solidity")

arrange_ggplot(p1, p2, p3)
```


